---
title: Kafka教程
tags: 消息中间件
layout: post
categories: 十五、消息中间件
---



## 一、概述

### 消息中间件定义

　　**中间件**（英语：Middleware），又译**中间件、中介层**，是一类提供系统软件和应用软件之间连接、便于软件各部件之间的沟通的软件，应用软件可以借助中间件在不同的技术架构之间共享信息与资源。中间件位于客户机服务器的操作系统之上，管理着计算资源和网络通信。

　　**消息中间件 **是基于队列与消息传递技术，在网络环境中为应用系统提供同步或异步、可靠的消息传输的支撑性软件系统。

### 为什么要使用消息中间件

　　在应用程序之间使用消息中间件的好处有：

　　**1、异步执行任务：**假设系统A要向系统B发送消息，如果没有消息中间件的话，系统A必须等待系统B接收完消息后才能继续执行。有了消息中间件，系统A就可以把消息发送到消息中间件中，然后执行自己的任务，系统B什么时候接收消息和系统A无关，实现了异步处理任务，提高了效率。

　　**2、系统间解耦：**有了消息中间件，即使系统B出了问题，系统A也不受影响。

　　**3、削峰填谷：**假设系统A在短时间内给系统B发送了大量消息，超过了系统B的处理能力，就会给系统A和系统B造成很大压力。有了消息中间件，系统A就可以把大量消息发送到消息中间件，系统B可以按照自己的能力去慢慢读取这些消息，这叫做削峰；在系统A的消息高峰期过去之后的一段时间，系统B仍然能以正常的速度读取积压的消息，这叫做填谷。

### Kafka基本概念

　　Kafka的基本概念如下：

　　**1、Broker：**一个独立的Kafka服务器被称为Broker。

　　**2、消息：**消息是Kafka中最基本的数据单元，由字节数组组成。

　　**3、批次：**批次就是一组消息，这些消息属于同一个主题和分区。

　　**4、主题(Topic)：**消息通过主题进行分类，主题内有多个分区，这些分区可能位于不同的Broker上。

　　**5、分区(Partition)：**消息以追加的方式写入分区，分区内的消息是有序的。Kafka通过分区来实现数据冗余和伸缩性，具体来说，每个分区可以有多个副本，这些副本位于不同的Broker上，因此同一条消息可以被拷贝到多个Broker上，实现了数据冗余；同一个主题内的多个分区可以分布在多个Broker上，实现了伸缩性。

　　**6、消息偏移量(Offset)：**表示一个分区内消息的索引位置，从0开始递增，并且不会变化。每个消费者组在消费主题时，每读取一条消息，都会将这条消息的Offset保存到Broker中，以此来记录消费的位置，这样当消费者重启后，就可以从该位置开始消费，避免重复消费。

　　**7、生产者(Producer)：**生产者是向主题发送消息的Kafka客户端，一个生产者可以向多个主题发送消息，并且可以将消息发送到指定的某个分区。

　　**8、消费者(Consumer)：**消费者是从主题中读取消息的Kafka客户端，一个消费者可以消费多个分区，但一个分区只能有一个消费者，如果分区数量小于消费者数量，剩下的消费者就会空闲。分区的消费者由Kafka自动分配，不能手动指定。

　　**9、消费者组(Consumer group)：**消费者组是多个消费者构成的一个群组，一个消费者组可以消费多个分区以实现高并发。

　　**10、再均衡(Rebalanced)：**消费者组内某个消费者实例出现故障后，其它消费者实例会自动重新分配订阅主题分区，分区再均衡是Kafka消费者端实现高可用的重要手段。

## 二、Kafka的Java客户端

### 生产者的基本使用

* #### 简单用法

　　使用Kafka的Java客户端之前，需要引入依赖：

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>2.5.0</version>
</dependency>
```

　　生产者的代码如下：

```java
public class Producer {
    public static void main(String[] args) throws Exception {
        // 1、设置基本属性
        Properties properties = new Properties();
        properties.setProperty("bootstrap.servers", "192.168.220.128:9092");
        properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        properties.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");

        KafkaProducer<String, String> kafkaProducer = new KafkaProducer<>(properties);
        // 2、设置主题和消息内容
        ProducerRecord<String, String> producerRecord = new ProducerRecord<>("studentTopic", "name", "jack");
        
        // 3、发送消息
        try{
            kafkaProducer.send(producerRecord);
        }finally{
            // 即使是异步发送消息，close()方法也会等待返回响应后执行
            kafkaProducer.close();
        }
        Thread.sleep(1000);
    }
}
```

* #### 同步发送消息

　　同步发送消息的代码如下：

```java
Future<RecordMetadata> future = kafkaProducer.send(producerRecord);
// get()方法会阻塞等待Broker返回响应，以此实现同步
RecordMetadata recordMetadata = future.get();
```

* #### 异步发送消息

　　异步发送消息的写法有两种，第一种是忽略异步响应的写法：

```java
// 和同步发送消息的区别在于，它没有调用get()方法
kafkaProducer.send(producerRecord);
```

　　第二种是可以在异步响应时执行一些操作：

```java
// 发送消息时传入一个回调函数，当异步响应时会调用这个函数
kafkaProducer.send(producerRecord, (recordMetadata, exception) -> {
    exception.printStackTrace();
});
```

　　Kafka异步发送消息的原理是，在主线程中创建一个守护线程，由这个守护线程去发送消息，这样主线程就不会被阻塞了。守护线程的特点是，当虚拟机中的所有非守护线程执行结束后，守护线程不管有没有执行完都会结束，所以异步发送消息时要注意不要让守护线程提前结束。

> warning "警告"
>
> Kafka异步发送消息时，需要获取元数据Metadata，如果因为Broker连接失败、Topic未创建等原因获取失败，也会阻塞主线程。

### 消费者的基本使用

* #### 简单用法

　　消费者的代码如下：

```java
public class Consumer {
    public static void main(String[] args) {
        // 1、设置基本属性
        Properties properties = new Properties();
        properties.put("bootstrap.servers","192.168.220.128:9092");
        properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        properties.put("value.deserializer","org.apache.kafka.common.serialization.StringDeserializer");
        properties.put("group.id","group1");

        KafkaConsumer<String, String> kafkaConsumer = new KafkaConsumer<>(properties);
		// 2、订阅主题
        kafkaConsumer.subscribe(Arrays.asList("CustomerCountry"));

        try {
            while (true) {
                // 3、使用poll(轮询)的方式，从主题中读取消息，每次调用poll()都会从最后消费的offset处开始读取，读取尽可能多的数据
                ConsumerRecords<String, String> consumerRecords = kafkaConsumer.poll(100);
                for (ConsumerRecord<String, String> consumerRecord : consumerRecords) {
                    String topic = consumerRecord.topic();
                    int partition = consumerRecord.partition();
                    long offset = consumerRecord.offset();
                    String key = consumerRecord.key();
                    String value = consumerRecord.value();
                }
            }
        }finally {
            kafkaConsumer.close();
        }
    }
}
```





